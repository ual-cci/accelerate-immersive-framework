{{#if isGuide}}
<div id = "tutorial-container">
<h1> Guide to {{model.name}} </h1>
{{#if isMMLL}}
<p class = "tutorial-text">
Machine listening is the attempt to make computers hear sound intelligently. We often emulate the human hearing system, though engineering may not mirror human anatomy, and may deviate from physiological function, including purely mathematical algorithms to extract some sort of further information from audio signals. The interest of the MIMIC project is in musical machine listening, that is, the computer understanding of musical audio signals, and the Musical Machine Listening Library introduced here (subsequently MMLL) is a javascript library to do just that, in the web browser.
</p>

<p class = "tutorial-text">
The library provides a variety of higher level musical listening facilities for computer music, such as onset detection, chord detection, beat tracking and auditory modelling. All the listening objects can run live, to benefit the creation of interative music systems and live electronic music compositions. They can also render audio faster than realtime if called outside of a live processing callback, suitable for the analysis of audio files for machine learning purposes. The library also includes analysis and resynthesis capability from the inverse Fourier transform and via the tracking phase vocoder (which identifies sinusoidal partial trails within audio signals).
</p>

<p class = "tutorial-text">
The first embeded example here is a feature extractor from live audio. You can choose as input either a sound file from your hard drive, or an attached microphone. Note that you will need to give permission for the microphone to run within a web browser, for security reasons. A single feature is extracted, the Sensory Dissonance (how rough sounding tha audio is, according to a perceptual model). If you selected an audio file, it will play back, but if you selected microphone the output audio will be silent to avoid feedback.
</p>

<iframe class = "embedded-tutorial-code" src={{concat url "/code/f6a258e2-35c4-6b08-0bbf-07f334de613a?embed=true&showCode=true" allow="microphone" scrolling="no"}} style="width: 100%; height: 125px; overflow: hidden;">what?</iframe>

<p class = "tutorial-text">
The second embeded example is a live spectral view, showing the results of a Fast Fourier Transform of successive snapshots of the input signal. The power spectrum and phase spectrum are both plotted, in linear frequency range. Most of the activity will tend to be on the left of the plot of the power spectrum, for normal audio sources, whose spectral content tends to drop off for higher frequencies. You can choose the gain for the output to hear or not hear the source signal (the default is silence).
</p>

<p><iframe class = "embedded-tutorial-code" src={{concat url "/code/38c2887a-f8f3-5959-324c-7c0f176c0db7?embed=true&showCode=true"}} allow="microphone" scrolling="no" style="width: 100%; height:350px; overflow: hidden;">what?</iframe></p>

<p class = "tutorial-text">
The third embeded example is an onset detector, which reacts to percussive events in the input signal. If you are using live microphone near a speaker you may find headphones work best, to avoid feedback effects. An onset is indicated by a flashing colour change; changing the threshold adjusts the sensitivity of detection.
</p>

<p><iframe class = "embedded-tutorial-code" src={{concat url "/code/fcdf62d8-a47b-1ddf-f16a-9cd09b328a65?embed=true&showCode=true"}} allow="microphone" scrolling="no" style="width: 100%; height: 250px; overflow: hidden;">what?</iframe></p>

<p class = "tutorial-text">
Once an input sound is analysed, you can synthesize output based on the features, work with machine learning to further classify or process inputs based on the features,and make generally responsive and interactive music systems for concerts, installations, websites, etc
</p>


<p class = "tutorial-text">
  MMLL was developed by <a href="http://composerprogrammer.com/index.html">Nick Collins</a> as part of the AHRC funded MIMIC project (Musically Intelligent Machines Interacting Creatively). MMLL is released under an MIT license, see the included COPYING.txt file. The source code is available at <a href="https://github.com/sicklincoln/MMLL">github</a> though you can use it straight away from a web page just by linking to the <a href="https://raw.githubusercontent.com/sicklincoln/MMLL/master/MMLL.js">MMLL.js</a> source code file. The Examples folder provides a test example for each listener currently available in the library.</p>

<p class = "tutorial-text">Some more developed examples available on codecircle are linked now.</p>

<p class = "tutorial-text">Spectral delay based on spectral resynthesis. The input is analysed by FFT, then particular spectral bins can be independently delayed and fed back on themselves to make a diffuse delayed filterbank.</p>

<iframe class = "embedded-tutorial-code" src={{concat url "/code/d5499af6-f4f3-2683-0c05-b700f1a9f1b1?embed=true&showCode=true"}} allow="microphone" scrolling="no" style="width: 100%; height: 650px; overflow: hidden;">what?</iframe>

<p class = "tutorial-text">bbcut, based on beat tracking. The input is cut up live into stuttering buffers, with the cut points determined by tracking of the primary metrical level in the music.</p>

<p class = "tutorial-text"><iframe class = "embedded-tutorial-code" src={{concat url "/code/5ed346fe-f7d5-b7ce-87a4-df6e352dbb4a?embed=true&showCode=true"}} allow="microphone" scrolling="no" style="width: 100%; height: 550px;  overflow: hidden;">what?</iframe></p>
{{/if}}
</div>s
{{else}}
{{#each model as |guide|}}
  <a href = {{guide.url}}>{{guide.name}}</a><br>
{{/each}}
{{/if}}
