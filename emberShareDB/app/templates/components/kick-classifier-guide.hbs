<h1> Object Recognition Classification Example </h1>
<p class = "tutorial-text">
  Here we use the MobileNet feature extractor to control this audio track with a model trained on objects you hold up to your webcam. Instructions below.
</p>
{{embedded-project docId = "a4c91621-199c-65b5-e355-2aadfc27c33f" height = "450px" manualLoad = false}}
<p class = "tutorial-text">
<ol>
    <li class = "exercise-list-item">Use the drop down menu labelled “Class:” at the top of the Learner.js controls to change the current class. You should hear the drum beat change. </li>
    <li class = "exercise-list-item">Change the Class back to 0. Pick a neutral position (standing/sitting in the middle of the screen). Press record, and after a 2 second delay, 2 seconds of this pose will be recorded into Class 0 of the dataset. When we are recording, everytime a new frame of video is analysed, we take those features (the input) and store them alongside the class label (in this case 0). You can always hit the "Mute" button on the sampler if you need break from the music while you work on your classifier.</li>
    <li class = "exercise-list-item">The MobileNet feature extractor is good at telling what different objects are in the picture. Pick 3 objects from around your desk that you can hold up to the camera. Remember, the model will have to learn to spot the differences between these objects so the more different they are, the better it will work.</li>
    <li class = "exercise-list-item">For each object, change the Class dropdown to a new class BEFORE you press record. Then record in some examples of you holding up that object. Record in a few 2-second runs, having more examples and more slight variations on each object will make your classifier more robust. For example, you might want to record examples of an object in slightly different locations / positions / rotations.</li>
    <li class = "exercise-list-item">When you have examples of all three classes, press Train. This will train the model then automatically Run when it is done. When the classifier is running, everytime a new frame of video is analysed, we take those features (the input) and run them through the classifier. It predicts which object it thinks you are holding and reacts accordingly</li>
</ol>
</p>
